{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.84 s\n"
     ]
    }
   ],
   "source": [
    "import doctest\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from allennlp.nn import util as nn_util\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.data_loaders import MultiProcessDataLoader\n",
    "from allennlp.data.samplers import BucketBatchSampler, MaxTokensBatchSampler\n",
    "\n",
    "sys.path.append(\"../../gts_test\")\n",
    "from src.train_and_evaluate import *\n",
    "from src.models import *\n",
    "from src.expressions_transfer import *\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from libs.dataset_readers.math23k_reader import Math23kReader\n",
    "from libs.dataset_readers.math23k_transformer_reader import Math23kTransformerReader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.84 ms\n"
     ]
    }
   ],
   "source": [
    "def read_vocab(path):\n",
    "    tokens: List[str] = []\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            token = line.rstrip()\n",
    "            tokens.append(token)\n",
    "    return set(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "1398\n",
      "time: 16.4 ms\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = load_raw_data(\"../../gts_test/data/raw_original.json\")\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## GTS part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer numbers...\n",
      "Indexing words...\n",
      "keep_words 586 / 2006 = 0.2921\n",
      "Indexed 589 words in input language, 16 words in output\n",
      "Number of training data 1118\n",
      "Number of testind data 280\n",
      "time: 690 ms\n"
     ]
    }
   ],
   "source": [
    "pairs, generate_nums, copy_nums = transfer_num(data)\n",
    "\n",
    "temp_pairs = []\n",
    "for p in pairs:\n",
    "    temp_pairs.append((p[0], from_infix_to_prefix(p[1]), p[2], p[3]))\n",
    "pairs = temp_pairs\n",
    "\n",
    "pairs_tested = pairs[:280]\n",
    "pairs_trained = pairs[280:]\n",
    "\n",
    "input_lang, output_lang, train_pairs, test_pairs = prepare_data(pairs_trained, pairs_tested, 5, generate_nums,\n",
    "                                                                copy_nums, tree=True)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "input_batches, input_lengths, output_batches, output_lengths, nums_batches, num_stack_batches, num_pos_batches, num_size_batches\\\n",
    " = prepare_train_batch(train_pairs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([3, 4, 5, 6, 7, 8, 1, 9, 10, 11, 12, 1, 9, 13, 14, 4, 5, 15, 16, 17, 18, 6, 19, 20, 21, 22, 23], 27, [0, 0, 5, 8, 7], 5, ['1.6', '0.5'], [6, 11], [])\n",
      "(['一', '台', '压路机', '的', '滚筒', '长', 'NUM', '米', '，', '直径', '是', 'NUM', '米', '．', '这', '台', '压路机', '滚动', '一周', '压', '过', '的', '路面', '=', '多少', '平方米', '？'], ['*', '*', '3.14', 'N1', 'N0'], ['1.6', '0.5'], [6, 11])\n",
      "1118\n",
      "time: 2.35 ms\n"
     ]
    }
   ],
   "source": [
    "print(train_pairs[0])\n",
    "print(pairs_trained[0])\n",
    "print(len(train_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*', '/', '+', '-', '^', '3.14', '1', 'N0', 'N1', 'N2', 'N3', 'N4', 'N5', 'N6', 'N7', 'UNK']\n",
      "time: 1.45 ms\n"
     ]
    }
   ],
   "source": [
    "print(output_lang.index2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Normal reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118\n",
      "TextField of length 27 with text: \n",
      " \t\t[一, 台, 压路机, 的, 滚筒, 长, <NUM>, 米, ，, 直径, 是, <NUM>, 米, ．, 这, 台, 压路机, 滚动, 一周, 压, 过, 的, 路面, =, 多少, 平方米,\n",
      "\t\t？]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n",
      "Instance with fields:\n",
      " \t source_tokens: TextField of length 27 with text: \n",
      " \t\t[一, 台, 压路机, 的, 滚筒, 长, <NUM>, 米, ，, 直径, 是, <NUM>, 米, ．, 这, 台, 压路机, 滚动, 一周, 压, 过, 的, 路面, =, 多少, 平方米,\n",
      "\t\t？]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t target_tokens: TextField of length 5 with text: \n",
      " \t\t[*, *, 3.14, <N1>, <N0>]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t metadata: MetadataField (print field.metadata to see specific information). \n",
      "\n",
      "time: 176 ms\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/geometry_5fold_raw/fold0_train.json\"\n",
    "\n",
    "reader = Math23kReader(num_token_type=\"NUM\")\n",
    "dataset = list(reader.read(\"../data/geometry_5fold_raw/fold0_train.json\"))\n",
    "print(len(dataset))\n",
    "print(dataset[0][\"source_tokens\"])\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9831fdb8ea6b4651a3326809c8856c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', max=1118.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e05674207fc4dbd922efe1b95eda55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UNK', 'PAD', 'NUM'}\n",
      "{'<NUM>', '@@PADDING@@', '@@UNKNOWN@@'}\n",
      "{'@@PADDING@@'}\n",
      "set()\n",
      "{'UNK', 'PAD', 'NUM'}\n",
      "{'<NUM>', '@@UNKNOWN@@'}\n",
      "time: 97.8 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check vocab\n",
    "\n",
    "# GTS vocab\n",
    "GTS_vocab = set(input_lang.index2word)\n",
    "\n",
    "# Allennlp Vocab\n",
    "vocab = Vocabulary.from_instances(dataset, min_count={\"tokens\":5}, pretrained_files={\n",
    "           \"target_vocab\": \"../extra_files/equation_vocab.txt\"}, only_include_pretrained_words= True)\n",
    "allennlp_vocab = set(vocab.get_token_to_index_vocabulary(namespace='tokens'))\n",
    "\n",
    "# File vocab\n",
    "file_vocab = read_vocab(\"../results/seq2tree/check_loss/fold0/vocabulary/tokens.txt\")\n",
    "\n",
    "# Diff\n",
    "print(GTS_vocab.difference(allennlp_vocab))\n",
    "print(allennlp_vocab.difference(GTS_vocab))\n",
    "print(allennlp_vocab.difference(file_vocab))\n",
    "print(file_vocab.difference(allennlp_vocab))\n",
    "print(GTS_vocab.difference(file_vocab))\n",
    "print(file_vocab.difference(GTS_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dadd4d93ba4928b57078f41e87fd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='loading instances', max=1.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118\n",
      "time: 537 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Default:\")\n",
    "data_loader = MultiProcessDataLoader(reader, path, shuffle=False, batch_size=1)\n",
    "data_loader.index_with(vocab)\n",
    "allennlp_data = list(data_loader)\n",
    "\n",
    "print(len(allennlp_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.75 ms\n"
     ]
    }
   ],
   "source": [
    "with open(\"../extra_files/id_mapping_arithmetic.json\",\"r\") as f:\n",
    "    mapp = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.96 ms\n"
     ]
    }
   ],
   "source": [
    "def generate_num_stack(metadata):\n",
    "\n",
    "    num_stack_batch = []\n",
    "    for prob_metadata in metadata:\n",
    "        num_stack = []\n",
    "        for word in prob_metadata[\"target_tokens\"]:\n",
    "            temp_num = []\n",
    "            flag_not = True\n",
    "            if (vocab.get_token_index(word, \"target_vocab\")\n",
    "                    == vocab.get_token_index(\"@@UNKNOWN@@\", \"target_vocab\")):\n",
    "                flag_not = False\n",
    "                for i, j in enumerate(prob_metadata[\"numbers\"]):\n",
    "                    if j == word:\n",
    "                        temp_num.append(i)\n",
    "\n",
    "            if not flag_not and len(temp_num) != 0:\n",
    "                num_stack.append(temp_num)\n",
    "            if not flag_not and len(temp_num) == 0:\n",
    "                num_stack.append(\n",
    "                    [_ for _ in range(len(prob_metadata[\"numbers\"]))])\n",
    "        num_stack.reverse()\n",
    "        num_stack_batch.append(num_stack)\n",
    "    return num_stack_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 102 ms\n"
     ]
    }
   ],
   "source": [
    "# 0. No shuffle\n",
    "# 1. Input length\n",
    "# 2. Convert to ids\n",
    "# 3. Numbers, positions, num_stack\n",
    "\n",
    "for i, j in zip(train_pairs[:], allennlp_data[:]):\n",
    "    \n",
    "    # Compare the text\n",
    "    gts_text = \" \".join([input_lang.index2word[x] for x in i[0]])\n",
    "    gts_length = i[1]\n",
    "    allen_text = [vocab.get_token_from_index(x.item()) for x in j[\"source_tokens\"][\"tokens\"][\"tokens\"][0]]\n",
    "    allen_length = len(allen_text)\n",
    "    allen_text = \" \".join(allen_text)\n",
    "    allen_text = allen_text.replace(\"<NUM>\", \"NUM\").replace(\"@@UNKNOWN@@\", \"UNK\")\n",
    "    assert gts_text == allen_text\n",
    "    assert gts_length == allen_length\n",
    "    \n",
    "    # Compare the equation\n",
    "    gts_equation_ids = i[2]\n",
    "    allen_equation_ids = j[\"target_tokens\"][\"tokens\"][\"tokens\"][0]\n",
    "    allen_equation_ids = [mapp[vocab.get_token_from_index(x.item(), \"target_vocab\")] for x in allen_equation_ids]\n",
    "    assert gts_equation_ids == allen_equation_ids\n",
    "\n",
    "    # Compare the numbers and positions\n",
    "    gts_numbers = i[4]\n",
    "    gts_positions = i[5]\n",
    "    allen_numbers = j[\"metadata\"][0][\"numbers\"]\n",
    "    allen_positions = j[\"metadata\"][0][\"positions\"]\n",
    "    assert gts_numbers == allen_numbers\n",
    "    assert gts_positions == allen_positions\n",
    "    \n",
    "    # Compare the num stack\n",
    "    gts_num_stack = i[6]\n",
    "    allen_copy_positions = generate_num_stack(j[\"metadata\"])[0]\n",
    "    assert gts_num_stack == allen_copy_positions\n",
    "    \n",
    "    \n",
    "#     print(gts_text )\n",
    "#     print(allen_text)\n",
    "#     print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bert reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118\n",
      "Instance with fields:\n",
      " \t source_tokens: TextField of length 41 with text: \n",
      " \t\t[[CLS], 一, 台, 压, 路, 机, 的, 滚, 筒, 长, <NUM>, 米, ，, 直, 径, 是, <NUM>, 米, ．, 这, 台, 压, 路, 机, 滚, 动, 一, 周, 压,\n",
      "\t\t过, 的, 路, 面, =, 多, 少, 平, 方, 米, ？, [SEP]]\n",
      " \t\tand TokenIndexers : {'tokens': 'PretrainedTransformerIndexer'} \n",
      " \t target_tokens: TextField of length 5 with text: \n",
      " \t\t[*, *, 3.14, <N1>, <N0>]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t metadata: MetadataField (print field.metadata to see specific information). \n",
      "\n",
      "{'id': '281', 'problem': '一 台 压路机 的 滚筒 长 1.6 米 ， 直径 是 0.5 米 ． 这 台 压路机 滚动 一周 压 过 的 路面 = 多少 平方米 ？', 'equation': 'x=3.14*0.5*1.6', 'answer': '2.512', 'numbers': ['1.6', '0.5'], 'positions': [10, 16], 'source_tokens': ['一', '台', '压路机', '的', '滚筒', '长', '<NUM>', '米', '，', '直径', '是', '<NUM>', '米', '．', '这', '台', '压路机', '滚动', '一周', '压', '过', '的', '路面', '=', '多少', '平方米', '？'], 'target_tokens': ['*', '*', '3.14', '<N1>', '<N0>']}\n",
      "time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "# path = \"../data/geometry_5fold_raw/fold0_train.json\"\n",
    "\n",
    "# reader = Math23kTransformerReader(num_token_type=\"NUM\")\n",
    "# dataset = list(reader.read(\"../data/geometry_5fold_raw/fold0_train.json\"))\n",
    "# print(len(dataset))\n",
    "# print(dataset[0])\n",
    "# print(dataset[0][\"metadata\"].human_readable_repr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
